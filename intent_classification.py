# -*- coding: utf-8 -*-
"""Intent_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vEVwMa9QYd_Xy6wo1WSZ558qERL6l8i7

LSTM
"""

import pandas as pd
train_data= pd.read_csv('atis_intents_train.csv',
                       names= ["target", "text"])

test_data= pd.read_csv('atis_intents_test.csv',
                       names= ["target", "text"])

train_data

train_data.groupby("target").count()

# Resampling is done by copying data where target are atis_flight_time and atis_quantity.Due to imbalanced data.
train_data= train_data.append(train_data.loc[train_data.target.isin(["atis_flight_time", "atis_quantity"]), :])

#converting categorical labels(atis_flight_time) into a special numerical representation.
from sklearn.preprocessing import OneHotEncoder as OHE

y_encoder= OHE().fit(np.array(train_data.target).reshape(-1,1))

ytr_encoded= y_encoder.transform(np.array(train_data.target).reshape(-1,1)).toarray()
yts_encoded= y_encoder.transform(np.array(test_data.target).reshape(-1,1)).toarray()

"""Text Preprocessing With NLTK and Tensorflow"""

import nltk
nltk.download('punkt')
nltk.download('stopwords')

"""Convert text to lowercase"""

train_data["lower_text"]= train_data.text.map(lambda x: x.lower())
test_data["lower_text"]= test_data.text.map(lambda x: x.lower())

"""Word Tokenize"""

from nltk import word_tokenize

train_data["tokenized"]= train_data.lower_text.map(word_tokenize)
test_data["tokenized"]= test_data.lower_text.map(word_tokenize)

"""Remove Stop Words"""

from nltk.corpus import stopwords
from string import punctuation

def remove_stop(strings, stop_list):
    classed= [s for s in strings if s not in stop_list]
    return classed

stop= stopwords.words("english")
stop_punc= list(set(punctuation))+ stop

train_data["selected"]= train_data.tokenized.map(lambda df: remove_stop(df, stop_punc))
test_data["selected"]= test_data.tokenized.map(lambda df: remove_stop(df, stop_punc))

"""Stemming"""

from nltk.stem import PorterStemmer

def normalize(text):
    return " ".join(text)

stemmer= PorterStemmer()

train_data["stemmed"]= train_data.selected.map(lambda xs: [stemmer.stem(x) for x in xs])
train_data["normalized"]= train_data.stemmed.apply(normalize)

test_data["stemmed"]= test_data.selected.map(lambda xs: [stemmer.stem(x) for x in xs])
test_data["normalized"]= test_data.stemmed.apply(normalize)

#Convert text data into numbers
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer= Tokenizer(num_words= 10000)
tokenizer.fit_on_texts(train_data.normalized)

tokenized_train= tokenizer.texts_to_sequences(train_data.normalized)
tokenized_test= tokenizer.texts_to_sequences(test_data.normalized)

tokenizer.word_index.keys().__len__()

"""Pad Text"""

#It ensures that all these sequences have a maximum length of 20 words.
#This is important because it prepares the data for training neural networks, which usually require inputs of the same length.

from tensorflow.keras.preprocessing.sequence import pad_sequences

train_padded= pad_sequences(tokenized_train, maxlen= 20, padding= "pre")
test_padded= pad_sequences(tokenized_test, maxlen= 20, padding= "pre")

train_padded.shape

"""Create X Matrix (samples, steps, wordlist)"""

#This function takes text data (with columns padded) and turns it into a 3D matrix for use in LSTM

def transform_x(data, tokenizer):
    output_shape= [data.shape[0],
                  data.shape[1],
                  tokenizer.word_index.keys().__len__()]
    results= np.zeros(output_shape)

    for i in range(data.shape[0]):
        for ii in range(data.shape[1]):
            results[i, ii, data[i,ii]-1]= 1
    return results

xtr_transformed= transform_x(train_padded, tokenizer)
xts_transformed= transform_x(test_padded, tokenizer)

"""LSTM Modelling"""

from tensorflow.keras.layers import Dense, LSTM, BatchNormalization, Dropout, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import CategoricalCrossentropy as CC
from tensorflow.keras.activations import relu, softmax
from tensorflow.keras.initializers import he_uniform, glorot_uniform
from tensorflow.keras.metrics import AUC
from tensorflow.keras import Model
from tensorflow.keras.regularizers import l2


class LSTMModel(object):

    def build_model(self, input_dim, output_shape, steps, dropout_rate, kernel_regularizer, bias_regularizer):
        input_layer= Input(shape= (steps, input_dim)) #Takes data

        #make lstm_layer
        lstm= LSTM(units= steps)(input_layer) #Takes input layer
        #A fully connected layer with a specified number of units, activation function, and regularizers.
        dense_1= Dense(output_shape, kernel_initializer= he_uniform(),
                       bias_initializer= "zeros",
                       kernel_regularizer= l2(l= kernel_regularizer),
                       bias_regularizer= l2(l= bias_regularizer))(lstm)
        x= BatchNormalization()(dense_1) # Normalizes the output of the dense layer
        x= relu(x) # Applies the Rectified Linear Unit (ReLU) activation function.
        x= Dropout(rate= dropout_rate)(x) # Helps prevent overfitting by randomly dropping a fraction of the inputs.
        o= Dense(output_shape, kernel_initializer= glorot_uniform(),
                 bias_initializer= "zeros",
                 kernel_regularizer= l2(l= kernel_regularizer),
                 bias_regularizer= l2(l= bias_regularizer))(dense_1)
        o= BatchNormalization()(o)
        output= softmax(o, axis= 1) # Applies a softmax activation function to produce the final output.

        loss= CC()
        metrics= AUC()
        optimizer= Adam()
        self.model= Model(inputs= [input_layer], outputs= [output])
        self.model.compile(optimizer= optimizer, loss= loss, metrics= [metrics])


    def train(self, x, y, validation_split, epochs):
        self.model.fit(x, y, validation_split= validation_split, epochs= epochs)

    def predict(self, x):
        return self.model.predict(x)

"""Build Model"""

steps= xtr_transformed.shape[1]
dim= xtr_transformed.shape[2]
output_shape= ytr_encoded.shape[1]

model= LSTMModel()
model.build_model(input_dim= dim,
                  output_shape= output_shape,
                  steps= steps,
                  dropout_rate= 0.5,
                  bias_regularizer= 0.3,
                  kernel_regularizer= 0.3)

model.train(xtr_transformed, ytr_encoded,
           0.2, 60)

"""Evaluation"""

#Train
from sklearn.metrics import classification_report

prediction= y_encoder.inverse_transform(model.predict(xtr_transformed))
print(classification_report(train_data.target, prediction))

#Test
from sklearn.metrics import classification_report

prediction_test= y_encoder.inverse_transform(model.predict(xts_transformed))
print(classification_report(test_data.target, prediction_test))

"""Kaggle - SVM"""

def load_data(path):
    df = pd.read_csv(path)
    df.columns = ['label', 'query']
    return df

df = load_data('atis_intents.csv')
df.head(10)

# print shape of our data
print("There are {} rows and {} columns".format(df.shape[0], df.shape[1]))

# explore unique labels
print(df.label.unique())

# explore which labels are the most and least common
df.label.value_counts()

# drop rows with multiple labels
df = df[df["label"].str.contains("#")==False]
df.label.value_counts()

!python -m spacy download en_core_web_md

# import libraries and load spacy English model
import spacy
import numpy as np

nlp = spacy.load("en_core_web_md")
print("Number of vectors: {}".format(nlp.vocab.vectors_length))

# load training and test datasets
training_df = load_data('atis_intents_train.csv')
training_df = training_df[training_df["label"].str.contains("#")==False]

sen_train = training_df['query'].tolist()
labels_train = training_df['label'].tolist()

test_df = load_data('atis_intents_test.csv')
test_df = test_df[test_df["label"].str.contains("#")==False]
sen_test = test_df['query'].tolist()
labels_test = test_df['label'].tolist()

train_size = len(labels_train)
test_size = len(labels_test)


print('Train data has {} rows and test data has {} rows'.format(train_size, test_size))

def encode_sentences(sentences):
    # Calculate number of sentences
    n_sentences = len(sentences)
    X = np.zeros((n_sentences, 300))

    # Iterate over the sentences
    for idx, sentence in enumerate(sentences):
        # Pass each sentence to the nlp object to create a document
        doc = nlp(sentence)
        # Save the document's .vector attribute to the corresponding row in
        # X
        X[idx, :] = doc.vector
    return X

train_X = encode_sentences(sen_train) #trainquery
test_X = encode_sentences(sen_test) #testquery

#Sentences are converted into vector representations using spacy
#train_X and test_X are vector representations of query

# encode labels i.e turn text labels into integer representation
from sklearn.preprocessing import LabelEncoder

# Instantiate label encoder object
# The LabelEncoder is used for converting categorical labels (text labels) into numerical representations.
le = LabelEncoder()

labels_test = le.fit_transform(labels_test)
labels_train = le.fit_transform(labels_train)

"""Intent classification with SVM"""

# Import SVC
from sklearn.svm import SVC
# Train the model
#Creating an instance of SVM and setting hyperparameter C = 1.(C - controls the regularization strength)
clf = SVC(C=1)
clf.fit(train_X, labels_train)

#Evaluating the performance of the classifier (clf)
def validate_clf(X,y):
    # Validate model on training set
    y_pred = clf.predict(X)

    # Count the number of correct predictions
    n_correct = 0
    for i in range(len(y)):
        if y_pred[i] == y[i]:
            n_correct += 1

    print("Predicted {} correctly out of {}".format(n_correct, len(y)))
    print("Model accuracy: {}%".format(round(n_correct/len(y)*100),2))

print('Validation on the train set results:')
validate_clf(train_X, labels_train)

# Validate model on test set
print('Validation on the test set results:')
validate_clf(test_X, labels_test)